{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf574171",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è 1. Environment Setup\n",
    "\n",
    "We will install the required packages and load the models from Hugging Face.\n",
    "\n",
    "Make sure to run this in an environment with GPU support for faster inference and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate datasets peft bitsandbytes sentencepiece\n",
    "!pip install -q opencv-python pytesseract torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea63ac",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ 2. Inference Demo using Donut and MiniCPM\n",
    "\n",
    "We will run inference on an invoice image (`invoice_sample.png`). Please replace it with your real data as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1a5fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "processor = DonutProcessor.from_pretrained(\"to-be/donut-base-finetuned-invoices\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"to-be/donut-base-finetuned-invoices\").to(device)\n",
    "\n",
    "# Load image\n",
    "image = Image.open(\"/Users/xiaotingzhou/Documents/Lectures/AI_OCR/data/converted_images/invoice_page_1.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Prepare input\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "task_prompt = \"<s>Invoice Information:\"\n",
    "\n",
    "# Inference\n",
    "outputs = model.generate(pixel_values, decoder_input_ids=processor.tokenizer(task_prompt, return_tensors=\"pt\").input_ids.to(device), max_length=512)\n",
    "result = processor.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164e15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data: Invoice Information:23-01-30</s_DocumentDate><s_GrossAmount> 61388.00</s_GrossAmount><s_InvoiceNumber> 309824263008</s_InvoiceNumber><s_NetAmount1> 6138872.00</s_NetAmount1><s_TaxAmount1> 0.00</s_TaxAmount1>\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracted Data:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adacddf9",
   "metadata": {},
   "source": [
    "# Change Prompt\n",
    "\n",
    "- Consistent Format : Training and inference use the same task prompt format\n",
    "- Simple Outputs : Instead of complex structured output, use simple \"prompt: value\" format\n",
    "- Task-Specific Data : Create separate training samples for each extraction task\n",
    "- Better Training : Improved batch size, learning rate, and epochs\n",
    "- Proper Testing : Test each task prompt individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4ae37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "        processed = processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = processed.pixel_values.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # FIXED: Use consistent task prompt format\n",
    "        task_prompt = \"<s>InvoiceNo:\"  # This will be your consistent prompt\n",
    "        ground_truth = example[\"ground_truth\"]\n",
    "        \n",
    "        # FIXED: Create target that matches your desired output format\n",
    "        # Instead of complex structured format, use simple key-value format\n",
    "        target_text = f\"{task_prompt} {ground_truth['InvoiceNo']}</s>\"\n",
    "        \n",
    "        # Tokenize target\n",
    "        tokenized = processor.tokenizer(target_text, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      padding=False,\n",
    "                                      truncation=True, \n",
    "                                      max_length=512)\n",
    "        labels = tokenized.input_ids.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65609329",
   "metadata": {},
   "source": [
    "## donut-finetuned-task-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d55cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task-specific training data created!\n",
      "üìÅ Train data: 5 samples\n",
      "üìÅ Validation data: 5 samples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/training', exist_ok=True)\n",
    "\n",
    "# FIXED: Create separate training samples for different tasks\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# Base invoice data\n",
    "base_invoice = {\n",
    "    \"InvoiceNo\": \"Y 309824263008\",\n",
    "    \"InvoiceDate\": \"2025Âπ¥6Êúà30Êó•\",\n",
    "    \"Currency\": \"USD\",\n",
    "    \"Amount with Tax\": \"300\",\n",
    "    \"Amount without Tax\": \"300\",\n",
    "    \"Tax\": \"0\"\n",
    "}\n",
    "\n",
    "# Create training samples for each field you want to extract\n",
    "fields_to_extract = {\n",
    "    \"InvoiceNo\": \"<s>InvoiceNo:\",\n",
    "    \"InvoiceDate\": \"<s>InvoiceDate:\", \n",
    "    \"Currency\": \"<s>Currency:\",\n",
    "    \"Amount with Tax\": \"<s>Amount:\",\n",
    "    \"Tax\": \"<s>Tax:\"\n",
    "}\n",
    "\n",
    "# Generate training data for each task\n",
    "for field_name, task_prompt in fields_to_extract.items():\n",
    "    # Create training sample\n",
    "    train_sample = {\n",
    "        \"image\": \"data/converted_images/invoice_page_1.jpg\",\n",
    "        \"ground_truth\": {field_name: base_invoice[field_name]},\n",
    "        \"task_prompt\": task_prompt  # Add task prompt to data\n",
    "    }\n",
    "    train_data.append(train_sample)\n",
    "    \n",
    "    # Create validation sample\n",
    "    val_sample = {\n",
    "        \"image\": \"data/converted_images/invoice_page_1.jpg\",\n",
    "        \"ground_truth\": {field_name: base_invoice[field_name]},\n",
    "        \"task_prompt\": task_prompt\n",
    "    }\n",
    "    val_data.append(val_sample)\n",
    "\n",
    "# Save the datasets\n",
    "with open('data/training/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open('data/training/val.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Task-specific training data created!\")\n",
    "print(f\"üìÅ Train data: {len(train_data)} samples\")\n",
    "print(f\"üìÅ Validation data: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b30ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_task_prompt(example):\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "        processed = processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = processed.pixel_values.squeeze(0)\n",
    "        \n",
    "        # Use the task prompt from the data\n",
    "        task_prompt = example[\"task_prompt\"]\n",
    "        ground_truth = example[\"ground_truth\"]\n",
    "        \n",
    "        # Get the field name and value\n",
    "        field_name = list(ground_truth.keys())[0]\n",
    "        field_value = ground_truth[field_name]\n",
    "        \n",
    "        # Create target text: task_prompt + field_value + end_token\n",
    "        target_text = f\"{task_prompt} {field_value}</s>\"\n",
    "        \n",
    "        # Tokenize target\n",
    "        tokenized = processor.tokenizer(target_text, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      padding=False,\n",
    "                                      truncation=True, \n",
    "                                      max_length=512)\n",
    "        labels = tokenized.input_ids.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"task_prompt\": task_prompt  # Keep for reference\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e28bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_task_prompt(example):\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "        processed = processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = processed.pixel_values.squeeze(0)\n",
    "        \n",
    "        # Use the task prompt from the data\n",
    "        task_prompt = example[\"task_prompt\"]\n",
    "        ground_truth = example[\"ground_truth\"]\n",
    "        \n",
    "        # Get the field name and value\n",
    "        field_name = list(ground_truth.keys())[0]\n",
    "        field_value = ground_truth[field_name]\n",
    "        \n",
    "        # Create target text: task_prompt + field_value + end_token\n",
    "        target_text = f\"{task_prompt} {field_value}</s>\"\n",
    "        \n",
    "        # Tokenize target\n",
    "        tokenized = processor.tokenizer(target_text, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      padding=False,\n",
    "                                      truncation=True, \n",
    "                                      max_length=512)\n",
    "        labels = tokenized.input_ids.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels,\n",
    "            \"task_prompt\": task_prompt  # Keep for reference\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f727ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaotingzhou/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_with_task_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m\n\u001b[1;32m     65\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/training/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/training/val.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m })\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 4. Apply preprocessing\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[43mpreprocess_with_task_prompt\u001b[49m, remove_columns\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m     72\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m val samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_with_task_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "# Force CPU usage\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Move model to CPU\n",
    "# model = model.to(device)\n",
    "# 1. Define DonutDataCollator\n",
    "@dataclass\n",
    "class DonutDataCollator:\n",
    "    \"\"\"Custom data collator for Donut model that handles pixel_values and labels\"\"\"\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        pixel_values = []\n",
    "        labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Ensure pixel_values is a tensor\n",
    "            pv = feature[\"pixel_values\"]\n",
    "            if isinstance(pv, list):\n",
    "                pv = torch.tensor(pv)\n",
    "            elif not isinstance(pv, torch.Tensor):\n",
    "                pv = torch.tensor(pv)\n",
    "            pixel_values.append(pv)\n",
    "            \n",
    "            # Ensure labels is a tensor\n",
    "            label = feature[\"labels\"]\n",
    "            if isinstance(label, list):\n",
    "                label = torch.tensor(label)\n",
    "            elif not isinstance(label, torch.Tensor):\n",
    "                label = torch.tensor(label)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Stack pixel_values\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        \n",
    "        # Pad labels to the same length\n",
    "        max_length = max(len(label) for label in labels)\n",
    "        padded_labels = []\n",
    "        \n",
    "        for label in labels:\n",
    "            if len(label) < max_length:\n",
    "                padded_label = torch.cat([\n",
    "                    label,\n",
    "                    torch.full((max_length - len(label),), -100, dtype=label.dtype)\n",
    "                ])\n",
    "            else:\n",
    "                padded_label = label\n",
    "            padded_labels.append(padded_label)\n",
    "        \n",
    "        labels = torch.stack(padded_labels)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# 2. Initialize data collator\n",
    "data_collator = DonutDataCollator()\n",
    "\n",
    "# 3. Load dataset (assuming you've already created the task-specific data)\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"data/training/train.json\", \n",
    "    \"validation\": \"data/training/val.json\"\n",
    "})\n",
    "\n",
    "# 4. Apply preprocessing\n",
    "dataset = dataset.map(preprocess_with_task_prompt, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x is not None)\n",
    "\n",
    "print(f\"‚úÖ Dataset processed: {len(dataset['train'])} train, {len(dataset['validation'])} val samples\")\n",
    "\n",
    "# 5. Training arguments\n",
    "print(\"üîÑ Switched to CPU training to avoid memory issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b8b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/zcq5ydb90d53q6sd6g4zy7f00000gn/T/ipykernel_5888/2213293054.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting memory-optimized training...\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Clear memory first\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Memory-optimized training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./donut-finetuned-task-specific\",\n",
    "    per_device_train_batch_size=1,  # Minimal batch size\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Maintain effective batch size\n",
    "    num_train_epochs=5,  # Reduced epochs\n",
    "    learning_rate=3e-5, (smaller)\n",
    "    warmup_steps=25,\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,  # Shorter sequences\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=0,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Initialize trainer with optimized settings\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting memory-optimized training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfde0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory cleared before training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müßπ Memory cleared before training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Then proceed with training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Clear memory before training\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"üßπ Memory cleared before training\")\n",
    "\n",
    "# Then proceed with training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1ed4c",
   "metadata": {},
   "source": [
    "## donut-finetuned-task-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86d3a7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VisionEncoderDecoderModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load your trained model - use checkpoint-4\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionEncoderDecoderModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./donut-finetuned-task-specific/checkpoint-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m finetuned_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m finetuned_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VisionEncoderDecoderModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your trained model - use checkpoint-4\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained(\"./donut-finetuned-task-specific/checkpoint-4\")\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6e648b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test different task prompts\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>InvoiceNo:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>InvoiceDate:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>Tax:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/converted_images/invoice_page_1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m processor(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_prompt \u001b[38;5;129;01min\u001b[39;00m test_prompts:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Test different task prompts\n",
    "test_prompts = [\n",
    "    \"<s>InvoiceNo:\",\n",
    "    \"<s>InvoiceDate:\",\n",
    "    \"<s>Currency:\",\n",
    "    \"<s>Amount:\",\n",
    "    \"<s>Tax:\"\n",
    "]\n",
    "\n",
    "image = Image.open(\"data/converted_images/invoice_page_1.jpg\").convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "for task_prompt in test_prompts:\n",
    "    print(f\"\\nüß™ Testing prompt: {task_prompt}\")\n",
    "    \n",
    "    # Generate with specific task prompt\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, \n",
    "                                          add_special_tokens=False, \n",
    "                                          return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=128,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    \n",
    "    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"üìÑ Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde829a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VisionEncoderDecoderModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load your trained model - use checkpoint-5 (likely the latest)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m finetuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionEncoderDecoderModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./donut-finetuned-task-specific/checkpoint-5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m finetuned_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m finetuned_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VisionEncoderDecoderModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your trained model - use checkpoint-5 (likely the latest)\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained(\"./donut-finetuned-task-specific/checkpoint-5\")\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e58a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test different task prompts\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>InvoiceNo:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>InvoiceDate:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>Tax:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/converted_images/invoice_page_1.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m processor(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_prompt \u001b[38;5;129;01min\u001b[39;00m test_prompts:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Test different task prompts\n",
    "test_prompts = [\n",
    "    \"<s>InvoiceNo:\",\n",
    "    \"<s>InvoiceDate:\",\n",
    "    \"<s>Currency:\",\n",
    "    \"<s>Amount:\",\n",
    "    \"<s>Tax:\"\n",
    "]\n",
    "\n",
    "image = Image.open(\"data/converted_images/invoice_page_1.jpg\").convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "for task_prompt in test_prompts:\n",
    "    print(f\"\\nüß™ Testing prompt: {task_prompt}\")\n",
    "    \n",
    "    # Generate with specific task prompt\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, \n",
    "                                          add_special_tokens=False, \n",
    "                                          return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=128,\n",
    "            num_beams=1,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    \n",
    "    result = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(f\"üìÑ Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0fe10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaotingzhou/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error loading MiniCPM model: name 'List' is not defined\n",
      "üí° Continuing with Donut model only for this demo.\n"
     ]
    }
   ],
   "source": [
    "# Fix the MiniCPM model loading by adding proper imports and error handling\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List  # Add this import to fix the NameError\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    model_name = \"openbmb/MiniCPM-Llama3-V-2_5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    # Use AutoModel instead of AutoModelForCausalLM for vision-language models\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # For MiniCPM, we need to use the chat interface with images\n",
    "    # This is a placeholder since we need actual image processing\n",
    "    print(\"‚úÖ MiniCPM model loaded successfully!\")\n",
    "    print(\"üí° Note: MiniCPM requires image input for proper inference.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading MiniCPM model: {e}\")\n",
    "    print(\"üí° Continuing with Donut model only for this demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057287d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error loading MiniCPM model: name 'List' is not defined\n",
      "üí° Continuing with Donut model only for this demo.\n"
     ]
    }
   ],
   "source": [
    "# Fix the MiniCPM model loading by adding proper imports and error handling\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List  # Add this import to fix the NameError\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    model_name = \"openbmb/MiniCPM-Llama3-V-2_5\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    # Use AutoModel instead of AutoModelForCausalLM for vision-language models\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # For MiniCPM, we need to use the chat interface with images\n",
    "    # This is a placeholder since we need actual image processing\n",
    "    print(\"‚úÖ MiniCPM model loaded successfully!\")\n",
    "    print(\"üí° Note: MiniCPM requires image input for proper inference.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading MiniCPM model: {e}\")\n",
    "    print(\"üí° Continuing with Donut model only for this demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e50dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training and validation datasets created!\n",
      "üìÅ Train data: 2 samples\n",
      "üìÅ Validation data: 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Create sample training and validation datasets\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/training', exist_ok=True)\n",
    "\n",
    "# Sample training data\n",
    "train_data = [\n",
    "    {\n",
    "        \"image\": \"data/converted_images/invoice_page_1.jpg\",\n",
    "        \"ground_truth\": {\n",
    "            \"InvoiceNo\": \"Y 309824263008\",\n",
    "            \"InvoiceDate\": \"2025Âπ¥6Êúà30Êó•\",\n",
    "            \"Currency\": \"USD\",\n",
    "            \"Amount with Tax\": \"300\",\n",
    "            \"Amount without Tax\": \"300\",\n",
    "            \"Tax\": \"0\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"image\": \"data/converted_images/invoice_page_1.jpg\",  # Using same image for demo\n",
    "        \"ground_truth\": {\n",
    "            \"InvoiceNo\": \"Y 309824263008\",\n",
    "            \"InvoiceDate\": \"2025Âπ¥6Êúà30Êó•\",\n",
    "            \"Currency\": \"USD\",\n",
    "            \"Amount with Tax\": \"300\",\n",
    "            \"Amount without Tax\": \"300\",\n",
    "            \"Tax\": \"0\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Sample validation data\n",
    "val_data = [\n",
    "    {\n",
    "        \"image\": \"data/converted_images/invoice_page_1.jpg\",\n",
    "        \"ground_truth\": {\n",
    "            \"InvoiceNo\": \"Y 309824263008\",\n",
    "            \"InvoiceDate\": \"2025Âπ¥6Êúà30Êó•\",\n",
    "            \"Currency\": \"USD\",\n",
    "            \"Amount with Tax\": \"300\",\n",
    "            \"Amount without Tax\": \"300\",\n",
    "            \"Tax\": \"0\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save the datasets\n",
    "with open('data/training/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open('data/training/val.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Training and validation datasets created!\")\n",
    "print(f\"üìÅ Train data: {len(train_data)} samples\")\n",
    "print(f\"üìÅ Validation data: {len(val_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd86eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2 examples [00:00, 105.54 examples/s]\n",
      "Generating validation split: 1 examples [00:00, 360.49 examples/s]\n",
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(task_prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m]), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: pixel_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: decoder_input_ids}\n\u001b[0;32m---> 17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Training setup\u001b[39;00m\n\u001b[1;32m     20\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m     21\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./donut-finetuned-invoices\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Optional: metric to determine best model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:944\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    942\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 944\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    966\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3501\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3500\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3501\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3502\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(example):\n\u001b[0;32m---> 11\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m processor(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpixel_values[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>Invoice Information:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load dataset with correct file paths\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"data/training/train.json\", \n",
    "    \"validation\": \"data/training/val.json\"\n",
    "})\n",
    "# Preprocessing\n",
    "def preprocess(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values[0]\n",
    "    task_prompt = \"<s>Invoice Information:\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt + str(example[\"ground_truth\"]), return_tensors=\"pt\").input_ids[0]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": decoder_input_ids}\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "# Training setup\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./donut-finetuned-invoices\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy\n",
    "    save_strategy=\"epoch\",  # Also add save_strategy for consistency\n",
    "    load_best_model_at_end=True,  # Optional: load best model at end\n",
    "    metric_for_best_model=\"eval_loss\",  # Optional: metric to determine best model\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  5.24 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.88 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.32s/ examples]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.31s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset processed: 2 train, 1 val samples\n",
      "üîç Checking data types:\n",
      "Sample 0:\n",
      "  pixel_values type: <class 'list'>, shape: N/A\n",
      "  labels type: <class 'list'>, shape: N/A\n",
      "Sample 1:\n",
      "  pixel_values type: <class 'list'>, shape: N/A\n",
      "  labels type: <class 'list'>, shape: N/A\n",
      "üöÄ Starting training with fixed data collator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/zcq5ydb90d53q6sd6g4zy7f00000gn/T/ipykernel_29296/22915866.py:135: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 01:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.0, metrics={'train_runtime': 76.07, 'train_samples_per_second': 0.079, 'train_steps_per_second': 0.039, 'total_flos': 3.75229398122496e+16, 'train_loss': 0.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from PIL import Image\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# 1. Fixed Custom Data Collator for Donut\n",
    "@dataclass\n",
    "class DonutDataCollator:\n",
    "    \"\"\"Custom data collator for Donut model that handles pixel_values and labels\"\"\"\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract pixel_values and labels\n",
    "        pixel_values = []\n",
    "        labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Ensure pixel_values is a tensor\n",
    "            pv = feature[\"pixel_values\"]\n",
    "            if isinstance(pv, list):\n",
    "                pv = torch.tensor(pv)\n",
    "            elif not isinstance(pv, torch.Tensor):\n",
    "                pv = torch.tensor(pv)\n",
    "            pixel_values.append(pv)\n",
    "            \n",
    "            # Ensure labels is a tensor\n",
    "            label = feature[\"labels\"]\n",
    "            if isinstance(label, list):\n",
    "                label = torch.tensor(label)\n",
    "            elif not isinstance(label, torch.Tensor):\n",
    "                label = torch.tensor(label)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # Stack pixel_values\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        \n",
    "        # Pad labels to the same length\n",
    "        max_length = max(len(label) for label in labels)\n",
    "        padded_labels = []\n",
    "        \n",
    "        for label in labels:\n",
    "            # Pad with -100 (ignored in loss computation)\n",
    "            if len(label) < max_length:\n",
    "                padded_label = torch.cat([\n",
    "                    label,\n",
    "                    torch.full((max_length - len(label),), -100, dtype=label.dtype)\n",
    "                ])\n",
    "            else:\n",
    "                padded_label = label\n",
    "            padded_labels.append(padded_label)\n",
    "        \n",
    "        labels = torch.stack(padded_labels)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# 2. Fixed preprocessing function\n",
    "def preprocess(example):\n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "        # Ensure we get a proper tensor\n",
    "        processed = processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = processed.pixel_values.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Create proper target format for Donut\n",
    "        ground_truth = example[\"ground_truth\"]\n",
    "        # Convert ground truth to Donut's expected format\n",
    "        target_text = f\"<s_InvoiceNumber>{ground_truth['InvoiceNo']}</s_InvoiceNumber><s_InvoiceDate>{ground_truth['InvoiceDate']}</s_InvoiceDate><s_Currency>{ground_truth['Currency']}</s_Currency><s_AmountWithTax>{ground_truth['Amount with Tax']}</s_AmountWithTax><s_AmountWithoutTax>{ground_truth['Amount without Tax']}</s_AmountWithoutTax><s_Tax>{ground_truth['Tax']}</s_Tax></s>\"\n",
    "        \n",
    "        # Tokenize target\n",
    "        tokenized = processor.tokenizer(target_text, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      padding=False,\n",
    "                                      truncation=True, \n",
    "                                      max_length=512)\n",
    "        labels = tokenized.input_ids.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. Load and preprocess dataset\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"data/training/train.json\", \n",
    "    \"validation\": \"data/training/val.json\"\n",
    "})\n",
    "\n",
    "# Apply preprocessing and filter out None values\n",
    "dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x is not None)\n",
    "\n",
    "print(f\"‚úÖ Dataset processed: {len(dataset['train'])} train, {len(dataset['validation'])} val samples\")\n",
    "\n",
    "# Debug: Check data types\n",
    "print(\"üîç Checking data types:\")\n",
    "for i, sample in enumerate(dataset[\"train\"]):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  pixel_values type: {type(sample['pixel_values'])}, shape: {sample['pixel_values'].shape if hasattr(sample['pixel_values'], 'shape') else 'N/A'}\")\n",
    "    print(f\"  labels type: {type(sample['labels'])}, shape: {sample['labels'].shape if hasattr(sample['labels'], 'shape') else 'N/A'}\")\n",
    "    if i >= 1:  # Only check first 2 samples\n",
    "        break\n",
    "\n",
    "# 4. Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./donut-finetuned-invoices-v2\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,  # Reduced for testing\n",
    "    learning_rate=1e-5,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=512,\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# 5. Initialize custom data collator\n",
    "data_collator = DonutDataCollator()\n",
    "\n",
    "# 6. Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training with fixed data collator...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3538fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading fine-tuned model from ./donut-finetuned-invoices-v2/checkpoint-3...\n",
      "üîÑ Loading processor from original model: naver-clova-ix/donut-base-finetuned-docvqa...\n",
      "‚úÖ Fine-tuned model and processor loaded successfully!\n",
      "üß™ Testing fine-tuned model on: data/converted_images/invoice_page_1.jpg\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Raw model output: <s_InvoiceNumber><s_InvoiceDate><s_Currency><s_AmountWithTax><s_AmountWithoutTax><s_Tax><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "==================================================\n",
      "üéØ Parsed prediction:\n",
      "  InvoiceNo: \n",
      "  InvoiceDate: \n",
      "  Currency: \n",
      "  Amount with Tax: \n",
      "  Amount without Tax: \n",
      "  Tax: \n",
      "==================================================\n",
      "üéØ Ground truth:\n",
      "  InvoiceNo: Y 309824263008\n",
      "  InvoiceDate: 2025Âπ¥6Êúà30Êó•\n",
      "  Currency: USD\n",
      "  Amount with Tax: 300\n",
      "  Amount without Tax: 300\n",
      "  Tax: 0\n",
      "==================================================\n",
      "‚ùå InvoiceNo: MISMATCH - Predicted: '', Expected: 'Y 309824263008'\n",
      "‚ùå InvoiceDate: MISMATCH - Predicted: '', Expected: '2025Âπ¥6Êúà30Êó•'\n",
      "‚ùå Currency: MISMATCH - Predicted: '', Expected: 'USD'\n",
      "‚ùå Amount with Tax: MISMATCH - Predicted: '', Expected: '300'\n",
      "‚ùå Amount without Tax: MISMATCH - Predicted: '', Expected: '300'\n",
      "‚ùå Tax: MISMATCH - Predicted: '', Expected: '0'\n",
      "\n",
      "üìä **ACTUAL MODEL PERFORMANCE:**\n",
      "   Correct fields: 0/6\n",
      "   Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for inference - CORRECTED VERSION\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Use the original model name for the processor\n",
    "original_model_name = \"naver-clova-ix/donut-base-finetuned-docvqa\"\n",
    "model_path = \"./donut-finetuned-invoices-v2/checkpoint-3\"  # Your fine-tuned model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"üîÑ Loading fine-tuned model from {model_path}...\")\n",
    "print(f\"üîÑ Loading processor from original model: {original_model_name}...\")\n",
    "\n",
    "# Load fine-tuned model but original processor\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "finetuned_processor = DonutProcessor.from_pretrained(original_model_name)  # Use original processor\n",
    "\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "print(\"‚úÖ Fine-tuned model and processor loaded successfully!\")\n",
    "\n",
    "# Function to extract invoice data using the fine-tuned model\n",
    "def predict_invoice_data(image_path, model, processor):\n",
    "    \"\"\"Extract invoice data using the fine-tuned Donut model\"\"\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Create task prompt matching your training format\n",
    "    task_prompt = \"<s_InvoiceNumber><s_InvoiceDate><s_Currency><s_AmountWithTax><s_AmountWithoutTax><s_Tax>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, \n",
    "                                          add_special_tokens=False, \n",
    "                                          return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=512,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Function to parse the model output into structured data\n",
    "def parse_donut_output(output_text):\n",
    "    \"\"\"Parse Donut model output into structured JSON\"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Define patterns for each field based on your training format\n",
    "    patterns = {\n",
    "        'InvoiceNo': r'<s_InvoiceNumber>(.*?)</s_InvoiceNumber>',\n",
    "        'InvoiceDate': r'<s_InvoiceDate>(.*?)</s_InvoiceDate>',\n",
    "        'Currency': r'<s_Currency>(.*?)</s_Currency>',\n",
    "        'Amount with Tax': r'<s_AmountWithTax>(.*?)</s_AmountWithTax>',\n",
    "        'Amount without Tax': r'<s_AmountWithoutTax>(.*?)</s_AmountWithoutTax>',\n",
    "        'Tax': r'<s_Tax>(.*?)</s_Tax>'\n",
    "    }\n",
    "    \n",
    "    # Extract each field\n",
    "    for field, pattern in patterns.items():\n",
    "        match = re.search(pattern, output_text)\n",
    "        if match:\n",
    "            result[field] = match.group(1).strip()\n",
    "        else:\n",
    "            result[field] = \"\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ADD THE MISSING FUNCTION HERE\n",
    "def calculate_field_accuracy(predicted, ground_truth):\n",
    "    \"\"\"Calculate field-level accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = len(ground_truth)\n",
    "    \n",
    "    for field in ground_truth:\n",
    "        pred_value = predicted.get(field, \"\").strip()\n",
    "        gt_value = str(ground_truth[field]).strip()\n",
    "        \n",
    "        if pred_value == gt_value:\n",
    "            correct += 1\n",
    "            print(f\"‚úÖ {field}: MATCH\")\n",
    "        else:\n",
    "            print(f\"‚ùå {field}: MISMATCH - Predicted: '{pred_value}', Expected: '{gt_value}'\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# Test the model on your training data\n",
    "test_image_path = \"data/converted_images/invoice_page_1.jpg\"\n",
    "\n",
    "print(f\"üß™ Testing fine-tuned model on: {test_image_path}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get prediction from fine-tuned model\n",
    "raw_prediction = predict_invoice_data(test_image_path, finetuned_model, finetuned_processor)\n",
    "print(f\"üìÑ Raw model output: {raw_prediction}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parse the prediction\n",
    "parsed_prediction = parse_donut_output(raw_prediction)\n",
    "print(\"üéØ Parsed prediction:\")\n",
    "for field, value in parsed_prediction.items():\n",
    "    print(f\"  {field}: {value}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load ground truth for comparison\n",
    "with open('data/training/val.json', 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "    ground_truth = val_data[0]['ground_truth']  # First validation sample\n",
    "\n",
    "print(\"üéØ Ground truth:\")\n",
    "for field, value in ground_truth.items():\n",
    "    print(f\"  {field}: {value}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate accuracy using the function\n",
    "accuracy, correct, total = calculate_field_accuracy(parsed_prediction, ground_truth)\n",
    "print(f\"\\nüìä **ACTUAL MODEL PERFORMANCE:**\")\n",
    "print(f\"   Correct fields: {correct}/{total}\")\n",
    "print(f\"   Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading fine-tuned model from ./donut-finetuned-invoices-v2/checkpoint-3...\n",
      "üîÑ Loading processor from original model: naver-clova-ix/donut-base-finetuned-docvqa...\n",
      "‚úÖ Fine-tuned model and processor loaded successfully!\n",
      "üß™ Testing fine-tuned model on: data/converted_images/invoice_page_1.jpg\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer =  <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "üìÑ Raw model output: <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n",
      "==================================================\n",
      "üéØ Parsed prediction:\n",
      "  InvoiceNo: \n",
      "  InvoiceDate: \n",
      "  Currency: \n",
      "  Amount with Tax: \n",
      "  Amount without Tax: \n",
      "  Tax: \n",
      "==================================================\n",
      "üéØ Ground truth:\n",
      "  InvoiceNo: Y 309824263008\n",
      "  InvoiceDate: 2025Âπ¥6Êúà30Êó•\n",
      "  Currency: USD\n",
      "  Amount with Tax: 300\n",
      "  Amount without Tax: 300\n",
      "  Tax: 0\n",
      "==================================================\n",
      "‚ùå InvoiceNo: MISMATCH - Predicted: '', Expected: 'Y 309824263008'\n",
      "‚ùå InvoiceDate: MISMATCH - Predicted: '', Expected: '2025Âπ¥6Êúà30Êó•'\n",
      "‚ùå Currency: MISMATCH - Predicted: '', Expected: 'USD'\n",
      "‚ùå Amount with Tax: MISMATCH - Predicted: '', Expected: '300'\n",
      "‚ùå Amount without Tax: MISMATCH - Predicted: '', Expected: '300'\n",
      "‚ùå Tax: MISMATCH - Predicted: '', Expected: '0'\n",
      "\n",
      "üìä **ACTUAL MODEL PERFORMANCE:**\n",
      "   Correct fields: 0/6\n",
      "   Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model for inference - CORRECTED VERSION\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
    "from PIL import Image\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Use the original model name for the processor\n",
    "original_model_name = \"naver-clova-ix/donut-base-finetuned-docvqa\"\n",
    "model_path = \"./donut-finetuned-invoices-v2/checkpoint-3\"  # Your fine-tuned model\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"üîÑ Loading fine-tuned model from {model_path}...\")\n",
    "print(f\"üîÑ Loading processor from original model: {original_model_name}...\")\n",
    "\n",
    "# Load fine-tuned model but original processor\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "finetuned_processor = DonutProcessor.from_pretrained(original_model_name)  # Use original processor\n",
    "\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.eval()\n",
    "print(\"‚úÖ Fine-tuned model and processor loaded successfully!\")\n",
    "\n",
    "# Function to extract invoice data using the fine-tuned model\n",
    "def predict_invoice_data(image_path, model, processor):\n",
    "    \"\"\"Extract invoice data using the fine-tuned Donut model\"\"\"\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    # Create task prompt matching your training format\n",
    "    question = \"What is PAYMENT TERM?\"\n",
    "    task_prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, \n",
    "                                          add_special_tokens=False, \n",
    "                                          return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=512,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    # Decode the output\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    answer = sequence.split(\"<s_answer>\")[1]\n",
    "    print('answer = ', answer)\n",
    "    return answer\n",
    "\n",
    "# Function to parse the model output into structured data\n",
    "def parse_donut_output(output_text):\n",
    "    \"\"\"Parse Donut model output into structured JSON\"\"\"\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Define patterns for each field based on your training format\n",
    "    patterns = {\n",
    "        'InvoiceNo': r'<s_InvoiceNumber>(.*?)</s_InvoiceNumber>',\n",
    "        'InvoiceDate': r'<s_InvoiceDate>(.*?)</s_InvoiceDate>',\n",
    "        'Currency': r'<s_Currency>(.*?)</s_Currency>',\n",
    "        'Amount with Tax': r'<s_AmountWithTax>(.*?)</s_AmountWithTax>',\n",
    "        'Amount without Tax': r'<s_AmountWithoutTax>(.*?)</s_AmountWithoutTax>',\n",
    "        'Tax': r'<s_Tax>(.*?)</s_Tax>'\n",
    "    }\n",
    "    \n",
    "    # Extract each field\n",
    "    for field, pattern in patterns.items():\n",
    "        match = re.search(pattern, output_text)\n",
    "        if match:\n",
    "            result[field] = match.group(1).strip()\n",
    "        else:\n",
    "            result[field] = \"\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ADD THE MISSING FUNCTION HERE\n",
    "def calculate_field_accuracy(predicted, ground_truth):\n",
    "    \"\"\"Calculate field-level accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = len(ground_truth)\n",
    "    \n",
    "    for field in ground_truth:\n",
    "        pred_value = predicted.get(field, \"\").strip()\n",
    "        gt_value = str(ground_truth[field]).strip()\n",
    "        \n",
    "        if pred_value == gt_value:\n",
    "            correct += 1\n",
    "            print(f\"‚úÖ {field}: MATCH\")\n",
    "        else:\n",
    "            print(f\"‚ùå {field}: MISMATCH - Predicted: '{pred_value}', Expected: '{gt_value}'\")\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy, correct, total\n",
    "\n",
    "# Test the model on your training data\n",
    "test_image_path = \"data/converted_images/invoice_page_1.jpg\"\n",
    "\n",
    "print(f\"üß™ Testing fine-tuned model on: {test_image_path}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get prediction from fine-tuned model\n",
    "raw_prediction = predict_invoice_data(test_image_path, finetuned_model, finetuned_processor)\n",
    "print(f\"üìÑ Raw model output: {raw_prediction}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parse the prediction\n",
    "parsed_prediction = parse_donut_output(raw_prediction)\n",
    "print(\"üéØ Parsed prediction:\")\n",
    "for field, value in parsed_prediction.items():\n",
    "    print(f\"  {field}: {value}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load ground truth for comparison\n",
    "with open('data/training/val.json', 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "    ground_truth = val_data[0]['ground_truth']  # First validation sample\n",
    "\n",
    "print(\"üéØ Ground truth:\")\n",
    "for field, value in ground_truth.items():\n",
    "    print(f\"  {field}: {value}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate accuracy using the function\n",
    "accuracy, correct, total = calculate_field_accuracy(parsed_prediction, ground_truth)\n",
    "print(f\"\\nüìä **ACTUAL MODEL PERFORMANCE:**\")\n",
    "print(f\"   Correct fields: {correct}/{total}\")\n",
    "print(f\"   Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa67f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a1a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b744db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552378f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8d95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704368bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dbf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b420f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350935de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb9756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951fc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cca89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6fa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fdf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c778b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d6858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2c542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
