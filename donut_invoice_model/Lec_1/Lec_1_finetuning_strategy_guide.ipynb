{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04ed877a",
   "metadata": {},
   "source": [
    "# 🤖 模型微调参数配置与训练策略指南（Transformers + Seq2SeqTrainer）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880368f3",
   "metadata": {},
   "source": [
    "\n",
    "## 🎯 1. 微调方法总览\n",
    "\n",
    "### ✅ 参数微调（Fine-tuning with hyperparameters）\n",
    "\n",
    "微调通常是在已有预训练模型上，使用新任务的数据对其参数进行更新。\n",
    "目标是保留预训练知识的同时学习新任务特征。\n",
    "\n",
    "**关键参数包括：**\n",
    "- `learning_rate`\n",
    "- `num_train_epochs`\n",
    "- `batch_size`\n",
    "- `weight_decay`\n",
    "- `gradient_accumulation_steps`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc49110",
   "metadata": {},
   "source": [
    "\n",
    "## 📊 2. 实时跟踪日志 & 训练监控（Logging and Monitoring）\n",
    "\n",
    "建议使用 `TensorBoard` 或 `WandB` 实时查看训练损失（loss）、验证指标、学习率变化等。\n",
    "\n",
    "```python\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./out\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",  # or 'wandb'\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768870d",
   "metadata": {},
   "source": [
    "\n",
    "## 🔁 3. 初始采用默认参数，结合 K-Fold 慢慢调参\n",
    "\n",
    "初始可采用默认参数快速跑一轮训练（或只用一折），查看训练和验证趋势。\n",
    "\n",
    "调参建议顺序：\n",
    "1. 学习率 `learning_rate`\n",
    "2. 批大小 `batch_size` / 梯度累积 `gradient_accumulation_steps`\n",
    "3. 保存/评估频率 `save_steps` / `eval_steps`\n",
    "4. 权重衰减 `weight_decay`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac9127",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ 4. 动态调整 batch-size 和 step-size\n",
    "\n",
    "```python\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2  # 等效于8个样本的批量\n",
    ")\n",
    "```\n",
    "\n",
    "- 显存不足时：减小 `batch_size` + 增大 `gradient_accumulation_steps`\n",
    "- 显存充足时：可适当增大 `batch_size` 提升训练效率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0118d",
   "metadata": {},
   "source": [
    "\n",
    "## ❓ 5. 将每个问题作为单独训练样本（用于问答模型）\n",
    "\n",
    "适用于 Donut / T5 / GPT 类模型结构化问答训练。\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"发票号码是什么？\",\n",
    "  \"context\": \"图像或文本内容\",\n",
    "  \"answer\": \"INV-2025-001\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e22ec",
   "metadata": {},
   "source": [
    "\n",
    "## 🧱 6. 过拟合对策（Overfitting）\n",
    "\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| 加强正则化 | 提高 `weight_decay`（如从 0.01 → 0.05） |\n",
    "| 提前停止训练 | 使用 `early stopping` |\n",
    "| 数据增强 | 如OCR中旋转、模糊图像等 |\n",
    "| 减小模型容量 | 用 `donut-base` 替代 `donut-large` |\n",
    "| 增加训练数据 | 提高泛化能力的最好方式 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a649b22",
   "metadata": {},
   "source": [
    "\n",
    "## 🧱 7. 欠拟合对策（Underfitting）\n",
    "\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| 增大学习率 | 从 `1e-5` → `5e-5` |\n",
    "| 增加训练轮数 | 3 → 10 |\n",
    "| 更大模型 | 从 `donut-small` → `donut-base` |\n",
    "| 减小正则化 | 减少 `weight_decay` |\n",
    "| 检查数据质量 | 输入可能被截断/压缩 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfef79",
   "metadata": {},
   "source": [
    "\n",
    "## 🖥️ 8. 根据硬件配置参数\n",
    "\n",
    "| 情况 | 建议 |\n",
    "|------|------|\n",
    "| GPU显存小（<8GB） | 降低 `batch_size`，用 `gradient_accumulation_steps` |\n",
    "| 无GPU / CPU训练 | 设置 `no_cuda=True`，减少轮数 |\n",
    "| 多卡训练 | 使用 `accelerate` 或 `deepspeed` |\n",
    "| 显存溢出 | 开启 `fp16=True`（混合精度） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125532c",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ 总结建议\n",
    "\n",
    "| 阶段 | 要点 |\n",
    "|------|------|\n",
    "| 准备阶段 | 数据格式 & 模型选择正确 |\n",
    "| 初步试验 | 用小数据、小轮次跑通 |\n",
    "| 参数调整 | 优先调 `learning_rate` 和 `batch_size` |\n",
    "| 验证监控 | 日志、early stop、tensorboard |\n",
    "| 过拟合控制 | `weight_decay`、早停、数据增强 |\n",
    "| 欠拟合修复 | 更多轮次、学习率、模型大小 |\n",
    "| 资源限制应对 | 梯度累积、混合精度、动态 batch |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}