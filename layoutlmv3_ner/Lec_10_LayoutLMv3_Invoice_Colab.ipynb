{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XTMay/AI_OCR/blob/main/layoutlmv3_ner/Lec_10_LayoutLMv3_Invoice_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c643476",
      "metadata": {
        "id": "0c643476"
      },
      "source": [
        "\n",
        "# LayoutLMv3 发票信息抽取\n",
        "\n",
        "本 Notebook 演示：\n",
        "\n",
        "- 合成最小发票数据集（含图像、OCR token、bbox、BIO 标签）；\n",
        "- 使用 **LayoutLMv3**（HuggingFace Transformers）进行 **微调**；\n",
        "- 评估（实体级 F1、报告）；\n",
        "- 可视化误差（预测 vs 真实，bbox 叠加）；\n",
        "- 简单 **主动学习**（基于不确定性采样的增量标注与再训练）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e897b75",
      "metadata": {
        "id": "8e897b75"
      },
      "source": [
        "## 0. 准备环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c3123b",
      "metadata": {
        "id": "f4c3123b"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%bash\n",
        "pip -q install -U pip\n",
        "pip -q install \"transformers>=4.43.0\" \"datasets>=2.20.0\" \"accelerate>=0.31.0\"                \"seqeval>=1.2.2\" \"evaluate>=0.4.2\" pillow matplotlib                \"huggingface_hub>=0.23.0\"\n",
        "python - << 'PY'\n",
        "import torch, platform\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Python:\", platform.python_version())\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715d936a",
      "metadata": {
        "id": "715d936a"
      },
      "source": [
        "## 1. 导入与全局配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff14383",
      "metadata": {
        "id": "5ff14383"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json, random, math, shutil\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    LayoutLMv3Processor,\n",
        "    LayoutLMv3ForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# 项目目录\n",
        "ROOT = Path.cwd() / \"invoice_demo\"\n",
        "IMG_DIR = ROOT / \"images\"\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUT_DIR  = ROOT / \"outputs\"\n",
        "for d in [IMG_DIR, DATA_DIR, OUT_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 标签集合（BIO）\n",
        "LABELS = [\"O\",\n",
        "          \"B-INV_NO\",\"I-INV_NO\",\n",
        "          \"B-AMT_TOTAL\",\"I-AMT_TOTAL\",\n",
        "          \"B-AMT_NET\",\"I-AMT_NET\",\n",
        "          \"B-CURRENCY\",\"I-CURRENCY\"]\n",
        "id2label = dict(enumerate(LABELS))\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "\n",
        "# Processor/Model 名称\n",
        "MODEL_NAME = \"microsoft/layoutlmv3-base\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a04a7ac",
      "metadata": {
        "id": "9a04a7ac"
      },
      "source": [
        "\n",
        "## 2. 生成最小合成发票数据集（含 OCR tokens + bbox + BIO 标签）\n",
        "\n",
        "说明：\n",
        "- 使用 `PIL` 合成简单发票图像（中英混合）；\n",
        "- 同时产出 token 序列、位置框（归一化到 [0,1000]）、标签；\n",
        "- 拆分为 `train/validation/test/unlabeled`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9970413d",
      "metadata": {
        "id": "9970413d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def draw_text_with_box(draw, xy, text, font, fill=(0,0,0)):\n",
        "    draw.text(xy, text, fill=fill, font=font)\n",
        "    bbox = draw.textbbox(xy, text, font=font)  # (left, top, right, bottom)\n",
        "    return bbox\n",
        "\n",
        "def normalize_bbox(bbox, width, height, scale=1000):\n",
        "    x0,y0,x1,y1 = bbox\n",
        "    x0 = max(0, min(scale, int(scale * x0 / width)))\n",
        "    y0 = max(0, min(scale, int(scale * y0 / height)))\n",
        "    x1 = max(0, min(scale, int(scale * x1 / width)))\n",
        "    y1 = max(0, min(scale, int(scale * y1 / height)))\n",
        "    return [x0,y0,x1,y1]\n",
        "\n",
        "# 尝试加载系统字体（Colab可能存在这些），没有就用默认\n",
        "def get_font(size=20):\n",
        "    try:\n",
        "        return ImageFont.truetype(\"DejaVuSans.ttf\", size)\n",
        "    except:\n",
        "        return ImageFont.load_default()\n",
        "\n",
        "random.seed(7)\n",
        "\n",
        "def synth_invoice(sample_id:int, lang=\"en\") -> Dict[str,Any]:\n",
        "    W,H = 1000, 1400\n",
        "    img = Image.new(\"RGB\", (W,H), (255,255,255))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font_h1 = get_font(28)\n",
        "    font = get_font(22)\n",
        "    y = 50\n",
        "\n",
        "    tokens, bboxes, labels = [], [], []\n",
        "\n",
        "    # 抬头\n",
        "    title = \"INVOICE\" if lang==\"en\" else \"发票\"\n",
        "    bbox = draw_text_with_box(draw, (50,y), title, font_h1)\n",
        "    y += 60\n",
        "\n",
        "    # 发票号\n",
        "    if lang==\"en\":\n",
        "        pieces = [(\"Invoice\",\"B-INV_NO\"), (\"No.\",\"I-INV_NO\"), (str(10000+sample_id),\"I-INV_NO\")]\n",
        "    else:\n",
        "        pieces = [(\"发票号码\",\"B-INV_NO\"), (str(10000+sample_id),\"I-INV_NO\")]\n",
        "    x=50\n",
        "    for t,l in pieces:\n",
        "        bbox = draw_text_with_box(draw, (x,y), t, font)\n",
        "        tokens.append(t)\n",
        "        bboxes.append(normalize_bbox(bbox, W,H))\n",
        "        labels.append(l)\n",
        "        x += (len(t)*12 + 25)\n",
        "    y += 50\n",
        "\n",
        "    # 表头（金额/币种）\n",
        "    if lang==\"en\":\n",
        "        headers = [(\"Amount (USD)\",\"B-AMT_TOTAL\"), (\"Net Amount\",\"B-AMT_NET\")]\n",
        "        cur_token = (\"USD\",\"B-CURRENCY\")\n",
        "    else:\n",
        "        headers = [(\"金额(人民币)\",\"B-AMT_TOTAL\"), (\"不含税金额\",\"B-AMT_NET\")]\n",
        "        cur_token = (\"人民币\",\"B-CURRENCY\")\n",
        "\n",
        "    x=50\n",
        "    for text,l in headers:\n",
        "        bbox = draw_text_with_box(draw, (x,y), text, font)\n",
        "        tokens.append(text)\n",
        "        bboxes.append(normalize_bbox(bbox, W,H))\n",
        "        labels.append(l)\n",
        "        x += 350\n",
        "    y += 40\n",
        "\n",
        "    # 金额行\n",
        "    amt_total = round(random.uniform(100, 2000), 2)\n",
        "    amt_net   = round(amt_total * random.uniform(0.7, 0.95), 2)\n",
        "\n",
        "    # 币种 token（单独给一个位置）\n",
        "    bbox = draw_text_with_box(draw, (50,y), cur_token[0], font)\n",
        "    tokens.append(cur_token[0])\n",
        "    bboxes.append(normalize_bbox(bbox, W,H))\n",
        "    labels.append(cur_token[1])\n",
        "\n",
        "    # 金额 token\n",
        "    bbox = draw_text_with_box(draw, (250,y), f\"{amt_total:,.2f}\", font)\n",
        "    tokens.append(f\"{amt_total:,.2f}\")\n",
        "    bboxes.append(normalize_bbox(bbox, W,H))\n",
        "    labels.append(\"I-AMT_TOTAL\")\n",
        "\n",
        "    bbox = draw_text_with_box(draw, (600,y), f\"{amt_net:,.2f}\", font)\n",
        "    tokens.append(f\"{amt_net:,.2f}\")\n",
        "    bboxes.append(normalize_bbox(bbox, W,H))\n",
        "    labels.append(\"I-AMT_NET\")\n",
        "\n",
        "    # 随机添加一些干扰字段\n",
        "    y += 60\n",
        "    noise = [\"Vendor\", \"Address\", \"Date\", \"备注\", \"编号\", \"税率\", \"Thank you!\", \"合计\"]\n",
        "    x=50\n",
        "    for _ in range(8):\n",
        "        t = random.choice(noise)\n",
        "        bbox = draw_text_with_box(draw, (x,y), t, font)\n",
        "        tokens.append(t)\n",
        "        bboxes.append(normalize_bbox(bbox, W,H))\n",
        "        labels.append(\"O\")\n",
        "        x += random.randint(120, 220)\n",
        "        if x > 800:\n",
        "            x = 50\n",
        "            y += 35\n",
        "\n",
        "    return img, tokens, bboxes, labels\n",
        "\n",
        "def build_dataset(n_train=40, n_val=10, n_test=10, n_unlabeled=20):\n",
        "    # 随机语言分布\n",
        "    langs = [\"en\",\"zh\"]\n",
        "    entries = {\"train\":[], \"validation\":[], \"test\":[], \"unlabeled\":[]}\n",
        "    counters = {\"train\":0, \"validation\":0, \"test\":0, \"unlabeled\":0}\n",
        "\n",
        "    def add_split(split, idx):\n",
        "        lang = random.choice(langs)\n",
        "        img, tokens, bboxes, labels = synth_invoice(idx, lang)\n",
        "        img_path = IMG_DIR / f\"{split}_{idx:04d}.png\"\n",
        "        img.save(img_path)\n",
        "        entries[split].append({\n",
        "            \"id\": f\"{split}_{idx:04d}\",\n",
        "            \"words\": tokens,\n",
        "            \"bboxes\": bboxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_path\": str(img_path),\n",
        "            \"lang\": lang\n",
        "        })\n",
        "        counters[split]+=1\n",
        "\n",
        "    for i in range(n_train): add_split(\"train\", i)\n",
        "    for i in range(n_val): add_split(\"validation\", 1000+i)\n",
        "    for i in range(n_test): add_split(\"test\", 2000+i)\n",
        "    for i in range(n_unlabeled): add_split(\"unlabeled\", 3000+i)\n",
        "\n",
        "    # 写 JSONL\n",
        "    for split in entries:\n",
        "        with open(DATA_DIR / f\"{split}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for ex in entries[split]:\n",
        "                f.write(json.dumps(ex, ensure_ascii=False)+\"\n",
        "\")\n",
        "\n",
        "    print(\"Data stats:\", counters)\n",
        "\n",
        "build_dataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cead7a46",
      "metadata": {
        "id": "cead7a46"
      },
      "source": [
        "## 3. 加载数据集并预处理（Processor：图像+token+bbox → 模型输入）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03df884",
      "metadata": {
        "id": "e03df884"
      },
      "outputs": [],
      "source": [
        "\n",
        "processor = LayoutLMv3Processor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def load_jsonl(path:Path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\": Dataset.from_list(load_jsonl(DATA_DIR/\"train.jsonl\")),\n",
        "    \"validation\": Dataset.from_list(load_jsonl(DATA_DIR/\"validation.jsonl\")),\n",
        "    \"test\": Dataset.from_list(load_jsonl(DATA_DIR/\"test.jsonl\")),\n",
        "    \"unlabeled\": Dataset.from_list(load_jsonl(DATA_DIR/\"unlabeled.jsonl\")),\n",
        "})\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def preprocess(batch):\n",
        "    images = [Image.open(p).convert(\"RGB\") for p in batch[\"image_path\"]]\n",
        "    word_labels = []\n",
        "    for labs in batch[\"labels\"]:\n",
        "        word_labels.append([label2id[l] for l in labs])\n",
        "\n",
        "    enc = processor(\n",
        "        images,\n",
        "        batch[\"words\"],\n",
        "        boxes=batch[\"bboxes\"],\n",
        "        word_labels=word_labels,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    return enc\n",
        "\n",
        "encoded = raw_datasets.map(preprocess, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
        "encoded.set_format(type=\"torch\")\n",
        "encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1226bbc0",
      "metadata": {
        "id": "1226bbc0"
      },
      "source": [
        "## 4. 加载模型并设置 Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392efdc1",
      "metadata": {
        "id": "392efdc1"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "def seqeval_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = preds.argmax(-1)\n",
        "\n",
        "    # 恢复到标签字符串（忽略 padding：-100）\n",
        "    true_labels = []\n",
        "    true_preds = []\n",
        "    for p, l in zip(preds, labels):\n",
        "        cur_true_l, cur_pred_l = [], []\n",
        "        for pi, li in zip(p, l):\n",
        "            if li == -100:\n",
        "                continue\n",
        "            cur_true_l.append(id2label[int(li)])\n",
        "            cur_pred_l.append(id2label[int(pi)])\n",
        "        true_labels.append(cur_true_l)\n",
        "        true_preds.append(cur_pred_l)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds)\n",
        "    }\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(OUT_DIR/\"run_base\"),\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=encoded[\"train\"],\n",
        "    eval_dataset=encoded[\"validation\"],\n",
        "    tokenizer=processor.tokenizer,\n",
        "    compute_metrics=seqeval_metrics\n",
        ")\n",
        "\n",
        "print(\"Ready to train.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87716770",
      "metadata": {
        "id": "87716770"
      },
      "source": [
        "## 5. 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc80fe4",
      "metadata": {
        "id": "4dc80fe4"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer.train()\n",
        "trainer.evaluate(encoded[\"test\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1eabdd8",
      "metadata": {
        "id": "c1eabdd8"
      },
      "source": [
        "## 6. 误差可视化（预测 vs 真实）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd93ac2",
      "metadata": {
        "id": "dfd93ac2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def visualize_example(idx=0, split=\"test\", k=50):\n",
        "    # 取原始样本\n",
        "    ex = raw_datasets[split][idx]\n",
        "    img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    W,H = img.size\n",
        "\n",
        "    # 预处理\n",
        "    enc = processor(\n",
        "        img, ex[\"words\"], boxes=ex[\"bboxes\"], word_labels=[[label2id[l] for l in ex[\"labels\"]]],\n",
        "        truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\"\n",
        "    )\n",
        "    enc = {k:v.to(model.device) for k,v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    pred_ids = logits.argmax(-1).cpu().numpy()[0]\n",
        "\n",
        "    # 还原非 padding 的部分\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    words = []\n",
        "    bboxes = []\n",
        "    for w, b, tl in zip(ex[\"words\"], ex[\"bboxes\"], ex[\"labels\"]):\n",
        "        words.append(w)\n",
        "        bboxes.append(b)\n",
        "        true_labels.append(tl)\n",
        "    # pred 需要从 input_ids 对齐，这里直接按非 -100 的标签长度截取\n",
        "    pred_labels = [id2label[int(pid)] for pid in pred_ids[:len(true_labels)]]\n",
        "\n",
        "    # 可视化前 k 个 token 的 bbox\n",
        "    for i,(w,b,pl,tl) in enumerate(zip(words, bboxes, pred_labels, true_labels)):\n",
        "        if i>=k: break\n",
        "        x0,y0,x1,y1 = [int(v/1000*W) if i%2==0 else int(v/1000*H) for i,v in enumerate(b)]\n",
        "        color = (0,200,0) if pl==tl else (255,0,0)\n",
        "        draw.rectangle([x0,y0,x1,y1], outline=color, width=2)\n",
        "        draw.text((x0, max(0,y0-14)), f\"{w} | pred:{pl} | true:{tl}\", fill=color)\n",
        "\n",
        "    plt.figure(figsize=(8,11))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_example(idx=0, split=\"test\", k=60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de19b942",
      "metadata": {
        "id": "de19b942"
      },
      "source": [
        "## 7. 主动学习示例（不确定性采样 → 回标 → 再训练）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967ba310",
      "metadata": {
        "id": "967ba310"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def entropy(p: np.ndarray, axis=-1, eps=1e-12):\n",
        "    p = np.clip(p, eps, 1.0)\n",
        "    return -np.sum(p*np.log(p), axis=axis)\n",
        "\n",
        "def select_uncertain_samples(dataset, top_k=5):\n",
        "    scores = []\n",
        "    for i in range(len(dataset)):\n",
        "        ex = dataset[i]\n",
        "        img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
        "        enc = processor(img, ex[\"words\"], boxes=ex[\"bboxes\"], truncation=True,\n",
        "                        padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "        enc = {k:v.to(model.device) for k,v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits    # [1, seq, C]\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "        # 选取前 len(words) 的 token 概率计算熵\n",
        "        seq_len = len(ex[\"words\"])\n",
        "        ent = entropy(probs[:seq_len], axis=-1).mean()\n",
        "        scores.append((ent, i))\n",
        "    scores.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [idx for _,idx in scores[:top_k]]\n",
        "\n",
        "# 选择不确定样本\n",
        "top_indices = select_uncertain_samples(raw_datasets[\"unlabeled\"], top_k=5)\n",
        "print(\"Selected indices from unlabeled:\", top_indices)\n",
        "\n",
        "# 模拟“回标”：本例合成数据已有 labels，直接加入训练集\n",
        "selected = [raw_datasets[\"unlabeled\"][i] for i in top_indices]\n",
        "new_train_list = list(raw_datasets[\"train\"]) + selected\n",
        "\n",
        "# 保存新训练集并重新编码\n",
        "with open(DATA_DIR/\"train_al.jsonl\",\"w\",encoding=\"utf-8\") as f:\n",
        "    for ex in new_train_list:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False)+\"\\n\")\n",
        "\n",
        "train_al = Dataset.from_list(new_train_list)\n",
        "encoded_al = train_al.map(preprocess, batched=True, remove_columns=train_al.column_names)\n",
        "encoded_al.set_format(type=\"torch\")\n",
        "\n",
        "# 继续训练（小步数演示）\n",
        "args_al = TrainingArguments(\n",
        "    output_dir=str(OUT_DIR/\"run_active_learning\"),\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    learning_rate=3e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.06,\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer_al = Trainer(\n",
        "    model=model,\n",
        "    args=args_al,\n",
        "    train_dataset=encoded_al,\n",
        "    eval_dataset=encoded[\"validation\"],\n",
        "    tokenizer=processor.tokenizer,\n",
        "    compute_metrics=seqeval_metrics\n",
        ")\n",
        "\n",
        "trainer_al.train()\n",
        "res = trainer_al.evaluate(encoded[\"test\"])\n",
        "print(\"After Active Learning - Test metrics:\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 不确定性采样（Uncertainty Sampling）\n",
        "\n",
        "top_indices = select_uncertain_samples(raw_datasets[\"unlabeled\"], top_k=5)\n",
        "\n",
        "\t•\t目的：从未标注的数据中挑选模型最“不确定”的样本，让人工标注后最大化提升模型性能。\n",
        "\t•\t实现方式：\n",
        "\t•\t先用当前模型 model 对每个未标注样本做推理。\n",
        "\t•\t计算每个 token 的预测概率 probs。\n",
        "\t•\t通过 熵 (entropy) 衡量不确定性：\n",
        "\n",
        "ent = entropy(probs[:seq_len], axis=-1).mean()\n",
        "\n",
        "\t•\t熵越大 → 模型越不确定。\n",
        "\n",
        "\t•\t挑选熵最高的 top-k 样本加入待标注列表。\n",
        "\n",
        "\n",
        "### 2. 回标（Labeling / Pseudo-Labeling）\n",
        "\n",
        "selected = [raw_datasets[\"unlabeled\"][i] for i in top_indices]\n",
        "new_train_list = list(raw_datasets[\"train\"]) + selected\n",
        "\n",
        "\t•\t目的：给挑选出的样本加上标签（这里示例是合成数据，已有 label，所以直接加入训练集）。\n",
        "\t•\t实际应用：\n",
        "\t•\t在真实场景中，这一步通常需要人工标注。\n",
        "\t•\t也可以用模型自身的预测做 伪标注 (pseudo-label)，尤其在半监督场景。\n",
        "\n",
        "### 3. 再训练（Model Retraining / Fine-tuning）\n",
        "\n",
        "trainer_al.train()\n",
        "res = trainer_al.evaluate(encoded[\"test\"])\n",
        "\n",
        "\t•\t目的：用新增标注数据 微调模型，提高模型在关键样本上的性能。\n",
        "\t•\t流程：\n",
        "\t1.\t将新的训练集编码 (encoded_al)。\n",
        "\t2.\t使用 Trainer 再训练模型若干 epoch。\n",
        "\t3.\t评估更新后的模型在测试集的效果。\n",
        "\n",
        "\n",
        "### 4. 总结：主动学习流程\n",
        "\n",
        "原始训练集 → 训练模型 → 用模型预测未标注数据 → 选出最不确定的样本 → 人工标注/伪标注 → 加入训练集 → 再训练模型 → 重复\n",
        "\n",
        "\t•\t优点：\n",
        "\t•\t避免随机标注大量数据，节省标注成本。\n",
        "\t•\t重点标注模型不确定的样本，可以更快提升模型性能。\n",
        "\t•\t关键概念：\n",
        "\t•\t不确定性采样：基于模型预测的不确定性挑选样本（本例用熵）。\n",
        "\t•\t回标 / 标注：给挑选样本加上正确 label。\n",
        "\t•\t再训练：微调模型，将新标注数据的知识吸收进模型。\n"
      ],
      "metadata": {
        "id": "dmH2f8nBjkhw"
      },
      "id": "dmH2f8nBjkhw"
    },
    {
      "cell_type": "markdown",
      "id": "bbba71a8",
      "metadata": {
        "id": "bbba71a8"
      },
      "source": [
        "## 8. 超参数小网格实验（演示版）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5530af1",
      "metadata": {
        "id": "d5530af1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def small_sweep(lrs=[5e-5,3e-5], epochs=[4,6]):\n",
        "    results = []\n",
        "    for lr in lrs:\n",
        "        for ep in epochs:\n",
        "            run_dir = OUT_DIR / f\"sweep_lr{lr}_ep{ep}\"\n",
        "            args_s = TrainingArguments(\n",
        "                output_dir=str(run_dir),\n",
        "                per_device_train_batch_size=2,\n",
        "                per_device_eval_batch_size=2,\n",
        "                learning_rate=lr,\n",
        "                num_train_epochs=ep,\n",
        "                weight_decay=0.01,\n",
        "                warmup_ratio=0.1,\n",
        "                logging_steps=20,\n",
        "                evaluation_strategy=\"epoch\",\n",
        "                save_strategy=\"epoch\",\n",
        "                load_best_model_at_end=True,\n",
        "                metric_for_best_model=\"f1\",\n",
        "                fp16=True\n",
        "            )\n",
        "            trainer_s = Trainer(\n",
        "                model=LayoutLMv3ForTokenClassification.from_pretrained(\n",
        "                    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id\n",
        "                ),\n",
        "                args=args_s,\n",
        "                train_dataset=encoded[\"train\"],\n",
        "                eval_dataset=encoded[\"validation\"],\n",
        "                tokenizer=processor.tokenizer,\n",
        "                compute_metrics=seqeval_metrics\n",
        "            )\n",
        "            trainer_s.train()\n",
        "            eval_res = trainer_s.evaluate(encoded[\"validation\"])\n",
        "            results.append({\"lr\":lr,\"epochs\":ep, **eval_res})\n",
        "            print(\"Config:\", lr, ep, \"=>\", eval_res)\n",
        "    return results\n",
        "\n",
        "# 仅演示，实际可按需运行（注释掉默认不跑）\n",
        "# sweep_results = small_sweep()\n",
        "# sweep_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1928f064",
      "metadata": {
        "id": "1928f064"
      },
      "source": [
        "## 9. 推理与字段组装（示例）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbba061",
      "metadata": {
        "id": "6bbba061"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_amount(s: str):\n",
        "    s = s.replace(\",\", \"\").replace(\" \", \"\")\n",
        "    s = s.replace(\"（\",\"(\").replace(\"）\",\")\")\n",
        "    neg = s.startswith(\"(\") and s.endswith(\")\")\n",
        "    s = s.strip(\"()\")\n",
        "    try:\n",
        "        v = float(s)\n",
        "        return -v if neg else v\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def infer_fields(words:List[str], labels:List[str]):\n",
        "    inv_no = []\n",
        "    amt_total_tokens, amt_net_tokens, currency_tokens = [], [], []\n",
        "\n",
        "    for w,l in zip(words, labels):\n",
        "        if l.endswith(\"INV_NO\") and l[0] in (\"B\",\"I\"): inv_no.append(w)\n",
        "        if l.endswith(\"AMT_TOTAL\") and l[0] in (\"B\",\"I\"): amt_total_tokens.append(w)\n",
        "        if l.endswith(\"AMT_NET\") and l[0] in (\"B\",\"I\"): amt_net_tokens.append(w)\n",
        "        if l.endswith(\"CURRENCY\") and l[0] in (\"B\",\"I\"): currency_tokens.append(w)\n",
        "\n",
        "    inv_no = \" \".join(inv_no).strip()\n",
        "    amt_total = normalize_amount(\"\".join(amt_total_tokens)) if amt_total_tokens else None\n",
        "    amt_net = normalize_amount(\"\".join(amt_net_tokens)) if amt_net_tokens else None\n",
        "    currency = \" \".join(currency_tokens).upper().replace(\"人民币\",\"CNY\").strip() if currency_tokens else \"\"\n",
        "\n",
        "    # 简单上下文兜底\n",
        "    if not currency:\n",
        "        ctx = \" \".join(words).upper()\n",
        "        if \"USD\" in ctx or \"$\" in ctx: currency = \"USD\"\n",
        "        if \"人民币\" in ctx or \"CNY\" in ctx or \"RMB\" in ctx or \"¥\" in ctx: currency = \"CNY\"\n",
        "\n",
        "    return {\n",
        "        \"invoice_number\": inv_no,\n",
        "        \"amount_with_tax\": amt_total,\n",
        "        \"amount_without_tax\": amt_net,\n",
        "        \"currency\": currency if currency else \"UNKNOWN\"\n",
        "    }\n",
        "\n",
        "def predict_example(idx=0, split=\"test\"):\n",
        "    ex = raw_datasets[split][idx]\n",
        "    img = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
        "    enc = processor(img, ex[\"words\"], boxes=ex[\"bboxes\"], truncation=True,\n",
        "                    padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**{k:v.to(model.device) for k,v in enc.items()}).logits\n",
        "    pred_ids = logits.argmax(-1).cpu().numpy()[0]\n",
        "    pred_labels = [id2label[int(pid)] for pid in pred_ids[:len(ex[\"words\"])]]\n",
        "\n",
        "    fields = infer_fields(ex[\"words\"], pred_labels)\n",
        "    print(\"Pred fields:\", fields)\n",
        "    return fields\n",
        "\n",
        "_ = predict_example(idx=0, split=\"test\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}