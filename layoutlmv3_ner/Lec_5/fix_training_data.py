#!/usr/bin/env python3
"""
Fix the training data by applying proper bbox normalization
"""
import json
import os

def fix_training_data():
    """Apply bbox normalization to existing training data"""
    
    # Image dimensions from the log (2481x3508)
    img_width = 2481
    img_height = 3508
    
    # Read the existing training data
    train_file = "data/training/demo_train.json"
    if not os.path.exists(train_file):
        print("âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    
    print(f"ğŸ”§ ä¿®å¤ {len(train_data)} ä¸ªè®­ç»ƒæ ·æœ¬çš„bboxåæ ‡")
    
    fixed_data = []
    total_entities = 0
    fixed_entities = 0
    
    for item in train_data:
        fixed_item = {
            'image_path': item['image_path'],
            'entities': []
        }
        
        for entity in item['entities']:
            bbox = entity['bbox']
            total_entities += 1
            
            # Check if bbox needs normalization (all coordinates should be in 0-1000 range after normalization)
            # If coordinates look like pixel values (large numbers relative to 1000), normalize them
            if len(bbox) == 4 and (max(bbox) > 500 or (max(bbox) < 1000 and max(bbox) > 100)):
                # Apply normalization
                x1, y1, x2, y2 = bbox
                
                norm_x1 = max(0, min(1000, int((x1 / img_width) * 1000)))
                norm_y1 = max(0, min(1000, int((y1 / img_height) * 1000)))
                norm_x2 = max(0, min(1000, int((x2 / img_width) * 1000)))
                norm_y2 = max(0, min(1000, int((y2 / img_height) * 1000)))
                
                # Ensure x2 > x1, y2 > y1
                if norm_x2 <= norm_x1:
                    norm_x2 = min(1000, norm_x1 + 1)
                if norm_y2 <= norm_y1:
                    norm_y2 = min(1000, norm_y1 + 1)
                
                normalized_bbox = [norm_x1, norm_y1, norm_x2, norm_y2]
                
                print(f"  ğŸ“ '{entity['text'][:20]}': {bbox} -> {normalized_bbox}")
                
                fixed_entity = {
                    'text': entity['text'],
                    'bbox': normalized_bbox,
                    'label': entity['label']
                }
                fixed_entities += 1
            else:
                # Keep original if already normalized
                fixed_entity = entity.copy()
            
            fixed_item['entities'].append(fixed_entity)
        
        fixed_data.append(fixed_item)
    
    # Save the fixed data
    with open(train_file, 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š ä¿®å¤å®Œæˆ:")
    print(f"  æ€»å®ä½“æ•°: {total_entities}")
    print(f"  ä¿®å¤å®ä½“æ•°: {fixed_entities}")
    print(f"  ä¿å­˜åˆ°: {train_file}")
    
    # Validate the fixed data
    with open(train_file, 'r', encoding='utf-8') as f:
        validation_data = json.load(f)
    
    all_valid = True
    for item in validation_data:
        for entity in item['entities']:
            bbox = entity['bbox']
            if not all(isinstance(coord, int) and 0 <= coord <= 1000 for coord in bbox):
                print(f"âŒ éªŒè¯å¤±è´¥: {bbox}")
                all_valid = False
                break
        if not all_valid:
            break
    
    if all_valid:
        print("âœ… æ‰€æœ‰bboxåæ ‡éƒ½åœ¨0-1000èŒƒå›´å†…ï¼")
        return True
    else:
        print("âŒ ä»æœ‰æ— æ•ˆçš„bboxåæ ‡")
        return False

if __name__ == '__main__':
    success = fix_training_data()
    if success:
        print("\nğŸ‰ è®­ç»ƒæ•°æ®ä¿®å¤æˆåŠŸï¼ç°åœ¨å¯ä»¥è¿è¡ŒLayoutLMè®­ç»ƒäº†ã€‚")
    else:
        print("\nâŒ è®­ç»ƒæ•°æ®ä¿®å¤å¤±è´¥")