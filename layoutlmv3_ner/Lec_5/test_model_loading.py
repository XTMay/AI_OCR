#!/usr/bin/env python3
"""
Test if the saved model can be loaded properly for inference
"""
import logging
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_model_loading():
    """Test loading the saved model"""
    model_path = "./models/final_tutorial_invoice_layoutlmv3"
    
    try:
        logger.info(f"ğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½: {model_path}")
        
        # Check if all required files exist
        required_files = [
            "config.json",
            "model.safetensors", 
            "tokenizer_config.json",
            "preprocessor_config.json"
        ]
        
        logger.info("ğŸ“‹ æ£€æŸ¥å¿…éœ€æ–‡ä»¶:")
        for file in required_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                logger.info(f"  âœ… {file}")
            else:
                logger.error(f"  âŒ {file} - ç¼ºå¤±")
                return False
        
        # Try to load the model components
        from transformers import (
            LayoutLMv3ForTokenClassification,
            LayoutLMv3Tokenizer,
            LayoutLMv3Processor
        )
        
        logger.info("ğŸ”§ åŠ è½½æ¨¡å‹ç»„ä»¶:")
        
        # Load tokenizer
        tokenizer = LayoutLMv3Tokenizer.from_pretrained(model_path)
        logger.info("  âœ… TokenizeråŠ è½½æˆåŠŸ")
        
        # Load model
        model = LayoutLMv3ForTokenClassification.from_pretrained(model_path)
        logger.info("  âœ… ModelåŠ è½½æˆåŠŸ")
        
        # Try to load processor
        try:
            processor = LayoutLMv3Processor.from_pretrained(model_path)
            logger.info("  âœ… ProcessoråŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"  âš ï¸ ProcessoråŠ è½½å¤±è´¥: {e}")
            logger.info("  ğŸ”§ è¿™å¯èƒ½ä¸ä¼šå½±å“æ¨ç†ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨tokenizer")
        
        logger.info("ğŸ‰ æ¨¡å‹åŠ è½½æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return False

def test_inference_pipeline():
    """Test if inference pipeline can use the model"""
    try:
        logger.info("ğŸ”® æµ‹è¯•æ¨ç†ç®¡é“...")
        
        from src.inference_pipeline import InvoiceInferencePipeline
        
        # Initialize inference pipeline
        model_path = "./models/final_tutorial_invoice_layoutlmv3"
        
        pipeline = InvoiceInferencePipeline(model_path)
        logger.info("  âœ… æ¨ç†ç®¡é“åˆå§‹åŒ–æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ¨ç†ç®¡é“æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == '__main__':
    logger.info("ğŸ§ª å¼€å§‹æ¨¡å‹åŠ è½½å’Œæ¨ç†æµ‹è¯•...")
    
    # Test model loading
    loading_success = test_model_loading()
    
    # Test inference pipeline if loading succeeded
    if loading_success:
        inference_success = test_inference_pipeline()
        
        if inference_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å¯ä»¥æ­£å¸¸ç”¨äºæ¨ç†ã€‚")
            print("ç°åœ¨å¯ä»¥ç»§ç»­è¿è¡Œtutorial_demo.pyçš„æ¨ç†éƒ¨åˆ†äº†ã€‚")
        else:
            print("\nâš ï¸ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½†æ¨ç†ç®¡é“æœ‰é—®é¢˜")
    else:
        print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")