import os
import json
import logging
from datetime import datetime
from src.data_processor import InvoiceDataProcessor
from src.layoutlm_trainer import LayoutLMv3Trainer, InvoiceLayoutDataset
from src.inference_pipeline import InvoiceInferencePipeline
from src.training_monitor import TrainingMonitor
from src.performance_diagnostics import PerformanceDiagnostics
from src.progressive_training import ProgressiveTrainingStrategy

class InvoiceOCRTutorial:
    """å‘ç¥¨OCRå®Œæ•´æ•™å­¦æ¼”ç¤º"""
    
    def __init__(self):
        self.setup_logging()
        self.config = self.load_config()
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'tutorial_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_config(self) -> dict:
        """åŠ è½½é…ç½®"""
        base_dir = "/Users/xiaotingzhou/Documents/Lectures/AI_OCR/layoutlmv3_ner/Lec_5"
        
        # ç”Ÿæˆæ‰€æœ‰PDFæ–‡ä»¶è·¯å¾„
        pdf_files = []
        label_files = []
        
        for i in range(1, 8):  # 1åˆ°7
            pdf_path = f"{base_dir}/data/raw/æ¸¬è©¦è‚¡ä»½æœ‰é™å…¬å¸_{i}.pdf"
            label_path = f"{base_dir}/data/training/label_{i}.json"
            
            pdf_files.append(pdf_path)
            label_files.append(label_path)
        
        return {
            "data_dir": f"{base_dir}/data",
            "pdf_files": pdf_files,
            "label_files": label_files,
            "model_output_dir": "./models/tutorial_invoice_layoutlmv3",
            "final_model_path": "./models/final_tutorial_invoice_layoutlmv3"
        }
    
    def run_complete_tutorial(self):
        """è¿è¡Œå®Œæ•´æ•™å­¦æ¼”ç¤º"""
        self.logger.info("ğŸ“ å¼€å§‹å‘ç¥¨OCRå®Œæ•´æ•™å­¦æ¼”ç¤º")
        
        try:
            # éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨æ€§
            self.validate_data_files()
            
            # ç¬¬ä¸€æ­¥ï¼šæ•°æ®é¢„å¤„ç†æ¼”ç¤º
            self.logger.info("\n" + "="*50)
            self.logger.info("ğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ•°æ®é¢„å¤„ç†æ¼”ç¤º")
            self.logger.info("="*50)
            
            processor = self.demonstrate_data_preprocessing()
            
            # ç¬¬äºŒæ­¥ï¼šæ•°æ®æ ‡æ³¨æ¼”ç¤º
            self.logger.info("\n" + "="*50)
            self.logger.info("ğŸ·ï¸ ç¬¬äºŒæ­¥ï¼šæ•°æ®æ ‡æ³¨æ¼”ç¤º")
            self.logger.info("="*50)
            
            annotations = self.demonstrate_annotation_process(processor)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è®­ç»ƒæ¼”ç¤º
            self.logger.info("\n" + "="*50)
            self.logger.info("ğŸš€ ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è®­ç»ƒæ¼”ç¤º")
            self.logger.info("="*50)
            
            model_path = self.demonstrate_model_training(annotations)
            
            # ç¬¬å››æ­¥ï¼šæ¨ç†æ¼”ç¤º
            self.logger.info("\n" + "="*50)
            self.logger.info("ğŸ”® ç¬¬å››æ­¥ï¼šæ¨ç†æ¼”ç¤º")
            self.logger.info("="*50)
            
            self.demonstrate_inference(model_path)
            
            # ç¬¬äº”æ­¥ï¼šæ€§èƒ½åˆ†æ
            self.logger.info("\n" + "="*50)
            self.logger.info("ğŸ“ˆ ç¬¬äº”æ­¥ï¼šæ€§èƒ½åˆ†æ")
            self.logger.info("="*50)
            
            self.demonstrate_performance_analysis()
            
            self.logger.info("\nğŸ‰ æ•™å­¦æ¼”ç¤ºå®Œæˆï¼")
            
        except Exception as e:
            self.logger.error(f"æ•™å­¦æ¼”ç¤ºå¤±è´¥: {e}")
            raise
    
    def validate_data_files(self):
        """éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨æ€§"""
        self.logger.info("ğŸ” éªŒè¯æ•°æ®æ–‡ä»¶...")
        
        missing_files = []
        
        # æ£€æŸ¥PDFæ–‡ä»¶
        for pdf_file in self.config["pdf_files"]:
            if not os.path.exists(pdf_file):
                missing_files.append(pdf_file)
        
        # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
        for label_file in self.config["label_files"]:
            if not os.path.exists(label_file):
                missing_files.append(label_file)
        
        if missing_files:
            self.logger.error("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:")
            for file in missing_files:
                self.logger.error(f"  - {file}")
            raise FileNotFoundError(f"ç¼ºå°‘ {len(missing_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        self.logger.info(f"âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶éªŒè¯é€šè¿‡ ({len(self.config['pdf_files'])} PDF + {len(self.config['label_files'])} æ ‡æ³¨)")
    
    def demonstrate_data_preprocessing(self) -> InvoiceDataProcessor:
        """æ¼”ç¤ºæ•°æ®é¢„å¤„ç†"""
        self.logger.info("åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨...")
        processor = InvoiceDataProcessor(self.config["data_dir"])
        
        # å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶
        all_image_paths = []
        
        self.logger.info("ğŸ“„ æ‰¹é‡PDFè½¬å›¾åƒæ¼”ç¤º:")
        for i, pdf_path in enumerate(self.config["pdf_files"], 1):
            self.logger.info(f"  å¤„ç†PDF {i}/11: {os.path.basename(pdf_path)}")
            try:
                image_paths = processor.pdf_to_images(pdf_path)
                all_image_paths.extend(image_paths)
                self.logger.info(f"    âœ… è½¬æ¢äº† {len(image_paths)} å¼ å›¾åƒ")
            except Exception as e:
                self.logger.error(f"    âŒ è½¬æ¢å¤±è´¥: {e}")
        
        self.logger.info(f"  ğŸ“¸ æ€»è®¡è½¬æ¢ {len(all_image_paths)} å¼ å›¾åƒ")
        
        # OCRæ¼”ç¤ºï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾åƒï¼‰
        self.logger.info("\nğŸ” OCRæ–‡æœ¬æå–æ¼”ç¤º:")
        if all_image_paths:
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            ocr_data = processor.extract_text_with_positions(all_image_paths[0])
            self.logger.info(f"  âœ… æå–åˆ° {len(ocr_data)} ä¸ªæ–‡æœ¬å—")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æœ¬å—ä½œä¸ºç¤ºä¾‹
            for i, item in enumerate(ocr_data[:3]):
                self.logger.info(f"    ğŸ“ æ–‡æœ¬å— {i+1}: '{item['text'][:20]}...' ç½®ä¿¡åº¦: {item['confidence']:.2f}")
        
        # åŠ è½½æ‰€æœ‰æ ‡æ³¨æ•°æ®
        self.logger.info("\nğŸ·ï¸ æ‰¹é‡æ ‡æ³¨æ•°æ®åŠ è½½æ¼”ç¤º:")
        all_labels = []
        for i, label_path in enumerate(self.config["label_files"], 1):
            try:
                with open(label_path, 'r', encoding='utf-8') as f:
                    label_data = json.load(f)
                all_labels.append(label_data)
                self.logger.info(f"  âœ… æ ‡æ³¨æ–‡ä»¶ {i}: {len(label_data)} ä¸ªå­—æ®µ")
            except Exception as e:
                self.logger.error(f"  âŒ åŠ è½½æ ‡æ³¨æ–‡ä»¶ {i} å¤±è´¥: {e}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ‡æ³¨æ–‡ä»¶çš„å†…å®¹ä½œä¸ºç¤ºä¾‹
        if all_labels:
            self.logger.info("\n  ğŸ“‹ æ ‡æ³¨å­—æ®µç¤ºä¾‹ (æ–‡ä»¶1):")
            for field, value in list(all_labels[0].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                self.logger.info(f"    ğŸ”– {field}: {value}")
        
        return processor
    
    def demonstrate_annotation_process(self, processor: InvoiceDataProcessor):
        """æ¼”ç¤ºæ ‡æ³¨è¿‡ç¨‹"""
        self.logger.info("ğŸ·ï¸ æ‰¹é‡åˆ›å»ºè®­ç»ƒæ ‡æ³¨...")
        
        all_annotations = []
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶å¯¹
        for i, (pdf_path, label_path) in enumerate(zip(self.config["pdf_files"], self.config["label_files"]), 1):
            try:
                self.logger.info(f"  å¤„ç†æ–‡ä»¶å¯¹ {i}/11...")
                
                # åŠ è½½æ ‡æ³¨æ•°æ®
                with open(label_path, 'r', encoding='utf-8') as f:
                    label_data = json.load(f)
                
                # è·å–å›¾åƒè·¯å¾„
                image_paths = processor.pdf_to_images(pdf_path)
                if not image_paths:
                    self.logger.warning(f"    âš ï¸ æ–‡ä»¶ {i} æ²¡æœ‰å¯ç”¨çš„å›¾åƒ")
                    continue
                
                # åˆ›å»ºè®­ç»ƒæ ‡æ³¨
                # ä¿®å¤ï¼šè°ƒæ•´å‚æ•°é¡ºåºï¼Œä¼ å…¥å›¾åƒè·¯å¾„åˆ—è¡¨è€Œä¸æ˜¯å•ä¸ªå›¾åƒ
                annotations = processor.create_training_annotations([image_paths[0]], label_data)
                all_annotations.extend(annotations)
                
                self.logger.info(f"    âœ… åˆ›å»ºäº† {len(annotations)} ä¸ªæ ‡æ³¨")
                
            except Exception as e:
                self.logger.error(f"    âŒ å¤„ç†æ–‡ä»¶å¯¹ {i} å¤±è´¥: {e}")
        
        self.logger.info(f"\n  ğŸ“Š æ€»è®¡åˆ›å»º {len(all_annotations)} ä¸ªæ ‡æ³¨")
        
        # æ˜¾ç¤ºæ ‡æ³¨ç»Ÿè®¡
        if all_annotations:
            label_counts = {}
            for ann in all_annotations:
                label = ann['label']
                label_counts[label] = label_counts.get(label, 0) + 1
            
            self.logger.info("  ğŸ“ˆ æ ‡æ³¨åˆ†å¸ƒ:")
            for label, count in label_counts.items():
                self.logger.info(f"    ğŸ·ï¸ {label}: {count} ä¸ª")
        
        # ä¿å­˜æ ‡æ³¨ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        self._save_annotations_to_folder(all_annotations)
        
        return all_annotations
    
    def _save_annotations_to_folder(self, annotations):
        """ä¿å­˜æ ‡æ³¨ç»“æœåˆ°annotationsæ–‡ä»¶å¤¹"""
        try:
            # åˆ›å»ºannotationsæ–‡ä»¶å¤¹
            annotations_dir = "/Users/xiaotingzhou/Documents/Lectures/AI_OCR/layoutlmv3_ner/Lec_5/data/annotation"
            os.makedirs(annotations_dir, exist_ok=True)
            
            # ä¿å­˜å®Œæ•´çš„æ ‡æ³¨æ•°æ®
            annotations_file = os.path.join(annotations_dir, f"all_annotations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
            # è½¬æ¢æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_annotations = []
            for ann in annotations:
                try:
                    # å®‰å…¨å¤„ç†bboxæ•°æ®
                    bbox = ann['bbox']
                    if isinstance(bbox, (list, tuple)):
                        # ç¡®ä¿bboxä¸­çš„æ‰€æœ‰å€¼éƒ½æ˜¯æœ‰æ•ˆçš„æ•°å­—
                        safe_bbox = []
                        for coord in bbox:
                            if isinstance(coord, (int, float)) and not (coord == float('inf') or coord == float('-inf') or coord != coord):  # æ£€æŸ¥nan
                                safe_bbox.append(float(coord))
                            else:
                                safe_bbox.append(0.0)  # ä½¿ç”¨é»˜è®¤å€¼æ›¿æ¢æ— æ•ˆåæ ‡
                    else:
                        safe_bbox = [0.0, 0.0, 100.0, 20.0]  # é»˜è®¤bbox
                    
                    serializable_ann = {
                        'image_path': str(ann['image_path']),
                        'text': str(ann['text']),
                        'label': str(ann['label']),
                        'bbox': safe_bbox,
                        'confidence': float(ann.get('confidence', 1.0))
                    }
                    serializable_annotations.append(serializable_ann)
                    
                except Exception as e:
                    self.logger.warning(f"  âš ï¸ è·³è¿‡æ— æ•ˆæ ‡æ³¨: {e}")
                    continue
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(annotations_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_annotations, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"  ğŸ’¾ æ ‡æ³¨ç»“æœå·²ä¿å­˜åˆ°: {annotations_file}")
            self.logger.info(f"  ğŸ“Š æˆåŠŸä¿å­˜ {len(serializable_annotations)} ä¸ªæ ‡æ³¨")
            
            # æŒ‰æ ‡ç­¾åˆ†ç±»ä¿å­˜
            label_groups = {}
            for ann in serializable_annotations:
                label = ann['label']
                if label not in label_groups:
                    label_groups[label] = []
                label_groups[label].append(ann)
            
            # ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
            for label, label_annotations in label_groups.items():
                label_file = os.path.join(annotations_dir, f"{label}_annotations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                with open(label_file, 'w', encoding='utf-8') as f:
                    json.dump(label_annotations, f, ensure_ascii=False, indent=2)
                self.logger.info(f"  ğŸ“ {label} æ ‡æ³¨å·²ä¿å­˜åˆ°: {label_file}")
            
            # åˆ›å»ºç»Ÿè®¡æ‘˜è¦
            summary = {
                "æ€»æ ‡æ³¨æ•°é‡": len(serializable_annotations),
                "æ ‡æ³¨åˆ†å¸ƒ": {label: len(anns) for label, anns in label_groups.items()},
                "åˆ›å»ºæ—¶é—´": datetime.now().isoformat(),
                "æ–‡ä»¶åˆ—è¡¨": {
                    "å®Œæ•´æ ‡æ³¨": annotations_file,
                    "åˆ†ç±»æ ‡æ³¨": {label: f"{label}_annotations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json" for label in label_groups.keys()}
                }
            }
            
            summary_file = os.path.join(annotations_dir, f"annotation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"  ğŸ“‹ æ ‡æ³¨æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
            
        except Exception as e:
            self.logger.error(f"  âŒ ä¿å­˜æ ‡æ³¨ç»“æœå¤±è´¥: {e}")
    
    def demonstrate_model_training(self, annotations):
        """æ¼”ç¤ºæ¨¡å‹è®­ç»ƒ"""
        self.logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒæ¼”ç¤º...")
        
        if not annotations:
            self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ ‡æ³¨æ•°æ®ï¼Œè·³è¿‡è®­ç»ƒ")
            return None
        
        try:
            # åˆå§‹åŒ–è®­ç»ƒå™¨
            trainer = LayoutLMv3Trainer()
            
            # åˆ›å»ºæ•°æ®é›†
            train_data = []
            for ann in annotations:
                # Convert numpy types to Python native types for JSON serialization
                bbox = ann['bbox']
                if hasattr(bbox, 'tolist'):
                    bbox = bbox.tolist()
                elif isinstance(bbox, (list, tuple)):
                    bbox = [int(x) if hasattr(x, 'item') else x for x in bbox]
                
                train_data.append({
                    'image_path': str(ann['image_path']),
                    'entities': [{
                        'text': str(ann['text']),
                        'bbox': bbox,
                        'label': str(ann['label'])
                    }]
                })
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            os.makedirs("data/training", exist_ok=True)
            with open("data/training/demo_train.json", 'w', encoding='utf-8') as f:
                json.dump(train_data, f, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = InvoiceLayoutDataset("data/training/demo_train.json", trainer.tokenizer)
            val_dataset = train_dataset  # æ¼”ç¤ºç”¨ï¼Œå®é™…åº”è¯¥åˆ†å¼€
            # train_datasetï¼Œ val_datasetï¼Œ test_dataset

            
            self.logger.info(f"  âœ… è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
            
            # è®¾ç½®è®­ç»ƒï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼Œå°‘é‡epochï¼‰
            trainer.setup_training(
                train_dataset, 
                val_dataset, 
                self.config["model_output_dir"],
                num_epochs=2  # å¢åŠ åˆ°2ä¸ªepochä»¥è·å¾—æ›´å¥½æ•ˆæœ
            )
            
            # å¼€å§‹è®­ç»ƒ
            self.logger.info("  ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰...")
            trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            trainer.save_model(self.config["final_model_path"])
            self.logger.info(f"  âœ… æ¨¡å‹ä¿å­˜åˆ°: {self.config['final_model_path']}")
            
            return self.config["final_model_path"]
            
        except Exception as e:
            self.logger.error(f"  âŒ è®­ç»ƒå¤±è´¥: {e}")
            return None
    
    def demonstrate_inference(self, model_path):
        """æ¼”ç¤ºæ¨ç†è¿‡ç¨‹"""
        self.logger.info("ğŸ”® å¼€å§‹æ‰¹é‡æ¨ç†æ¼”ç¤º...")
        
        if not model_path or not os.path.exists(model_path):
            self.logger.warning("âš ï¸ æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¨ç†æ¼”ç¤º")
            return
        
        try:
            # åˆå§‹åŒ–æ¨ç†ç®¡é“
            inference_pipeline = InvoiceInferencePipeline(model_path)
            
            total_accuracy = 0
            successful_inferences = 0
            
            # åˆ›å»ºæ¨ç†ç»“æœä¿å­˜ç›®å½•
            inference_results_dir = "/Users/xiaotingzhou/Documents/Lectures/AI_OCR/layoutlmv3_ner/Lec_5/data/inference_results"
            os.makedirs(inference_results_dir, exist_ok=True)
            
            # å¤„ç†æ‰€æœ‰å‘ç¥¨
            for i, (pdf_path, label_path) in enumerate(zip(self.config["pdf_files"], self.config["label_files"]), 1):
                self.logger.info(f"\n  ğŸ“„ å¤„ç†å‘ç¥¨ {i}/11: {os.path.basename(pdf_path)}")
                
                try:
                    # æ¨ç†
                    invoice_info = inference_pipeline.process_invoice(pdf_path)
                    
                    # ä¿å­˜æ¨ç†ç»“æœåˆ°JSONæ–‡ä»¶
                    pdf_name = os.path.basename(pdf_path).replace('.pdf', '')
                    inference_result_file = os.path.join(inference_results_dir, f"inference_{pdf_name}.json")
                    
                    with open(inference_result_file, 'w', encoding='utf-8') as f:
                        json.dump(invoice_info, f, ensure_ascii=False, indent=2)
                    
                    self.logger.info(f"    ğŸ’¾ æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {inference_result_file}")
                    
                    if invoice_info and isinstance(invoice_info, dict):
                        # åŠ è½½ground truth
                        with open(label_path, 'r', encoding='utf-8') as f:
                            ground_truth = json.load(f)
                        
                        # åˆ›å»ºå¯¹æ¯”ç»“æœ
                        comparison_result = {
                            "pdf_file": pdf_name,
                            "inference_results": invoice_info,
                            "ground_truth": ground_truth,
                            "field_comparison": {},
                            "accuracy_metrics": {}
                        }
                        
                        # è®¡ç®—å‡†ç¡®æ€§
                        correct_fields = 0
                        total_fields = len(ground_truth)
                        
                        for field, true_value in ground_truth.items():
                            pred_value = invoice_info.get(field, "")
                            is_correct = str(pred_value).strip() == str(true_value).strip()
                            
                            comparison_result["field_comparison"][field] = {
                                "predicted": str(pred_value).strip(),
                                "ground_truth": str(true_value).strip(),
                                "correct": is_correct
                            }
                            
                            if is_correct:
                                correct_fields += 1
                        
                        accuracy = correct_fields / total_fields if total_fields > 0 else 0
                        comparison_result["accuracy_metrics"] = {
                            "correct_fields": correct_fields,
                            "total_fields": total_fields,
                            "accuracy": accuracy
                        }
                        
                        # ä¿å­˜å¯¹æ¯”ç»“æœ
                        comparison_file = os.path.join(inference_results_dir, f"comparison_{pdf_name}.json")
                        with open(comparison_file, 'w', encoding='utf-8') as f:
                            json.dump(comparison_result, f, ensure_ascii=False, indent=2)
                        
                        self.logger.info(f"    ğŸ“Š å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_file}")
                        
                        total_accuracy += accuracy
                        successful_inferences += 1
                        
                        self.logger.info(f"    âœ… æ¨ç†æˆåŠŸï¼Œå‡†ç¡®ç‡: {accuracy:.2%} ({correct_fields}/{total_fields})")
                        
                    else:
                        self.logger.error(f"    âŒ æ¨ç†å¤±è´¥: {invoice_info}")
                        
                        # ä¿å­˜å¤±è´¥ç»“æœ
                        failure_result = {
                            "pdf_file": pdf_name,
                            "status": "failed",
                            "result": invoice_info,
                            "error": "æ¨ç†è¿”å›äº†æ— æ•ˆç»“æœ"
                        }
                        
                        failure_file = os.path.join(inference_results_dir, f"failure_{pdf_name}.json")
                        with open(failure_file, 'w', encoding='utf-8') as f:
                            json.dump(failure_result, f, ensure_ascii=False, indent=2)
                        
                except Exception as e:
                    self.logger.error(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            
            # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
            if successful_inferences > 0:
                avg_accuracy = total_accuracy / successful_inferences
                self.logger.info(f"\n  ğŸ¯ å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2%} (æˆåŠŸå¤„ç† {successful_inferences}/7 ä¸ªæ–‡ä»¶)")
                
                # åˆ›å»ºæ€»ç»“æŠ¥å‘Š
                summary_report = {
                    "inference_summary": {
                        "total_files": len(self.config["pdf_files"]),
                        "successful_inferences": successful_inferences,
                        "failed_inferences": len(self.config["pdf_files"]) - successful_inferences,
                        "average_accuracy": avg_accuracy,
                        "total_accuracy": total_accuracy
                    },
                    "file_details": [],
                    "inference_results_directory": inference_results_dir,
                    "ground_truth_directory": "/Users/xiaotingzhou/Documents/Lectures/AI_OCR/layoutlmv3_ner/Lec_5/data/training",
                    "timestamp": datetime.now().isoformat()
                }
                
                # æ·»åŠ æ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
                for i, (pdf_path, label_path) in enumerate(zip(self.config["pdf_files"], self.config["label_files"]), 1):
                    pdf_name = os.path.basename(pdf_path).replace('.pdf', '')
                    summary_report["file_details"].append({
                        "file_number": i,
                        "pdf_file": pdf_name,
                        "inference_result_file": f"inference_{pdf_name}.json",
                        "comparison_file": f"comparison_{pdf_name}.json",
                        "ground_truth_file": os.path.basename(label_path)
                    })
                
                # ä¿å­˜æ€»ç»“æŠ¥å‘Š
                summary_file = os.path.join(inference_results_dir, "inference_summary_report.json")
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary_report, f, ensure_ascii=False, indent=2)
                
                self.logger.info(f"  ğŸ“‹ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")
                self.logger.info(f"  ğŸ“ æ‰€æœ‰æ¨ç†ç»“æœä¿å­˜åœ¨: {inference_results_dir}")
                
            else:
                self.logger.warning("  âš ï¸ æ²¡æœ‰æˆåŠŸçš„æ¨ç†ç»“æœ")
                
        except Exception as e:
            self.logger.error(f"  âŒ æ¨ç†æ¼”ç¤ºå¤±è´¥: {e}")
    
    def demonstrate_performance_analysis(self):
        """æ¼”ç¤ºæ€§èƒ½åˆ†æ"""
        self.logger.info("ğŸ“ˆ æ€§èƒ½åˆ†ææ¼”ç¤º...")
        
        try:
            # åˆå§‹åŒ–æ€§èƒ½è¯Šæ–­å™¨
            diagnostics = PerformanceDiagnostics()
            
            # åˆ†æç¬¬ä¸€ä¸ªPDFçš„OCRæ€§èƒ½ä½œä¸ºç¤ºä¾‹
            self.logger.info("  ğŸ” OCRæ€§èƒ½åˆ†æ (ç¤ºä¾‹æ–‡ä»¶):")
            ocr_metrics = diagnostics.analyze_ocr_performance(self.config["pdf_files"][0])
            
            for metric, value in ocr_metrics.items():
                self.logger.info(f"    ğŸ“Š {metric}: {value}")
            # ROC-AUC, epcho/bach-loss
            
            # æä¾›ä¼˜åŒ–å»ºè®®
            self.logger.info("\n  ğŸ’¡ åŸºäºå¤šæ–‡ä»¶æ•°æ®çš„ä¼˜åŒ–å»ºè®®:")
            suggestions = [
                "åˆ©ç”¨11ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§è¿›è¡Œæ•°æ®å¢å¼º",
                "åˆ†æä¸åŒå‘ç¥¨æ ¼å¼çš„å…±åŒç‰¹å¾å’Œå·®å¼‚",
                "ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ ·æœ¬ä¸Šçš„è¡¨ç°",
                "å®æ–½é›†æˆå­¦ä¹ ä»¥æé«˜å¯¹æ ¼å¼å˜åŒ–çš„é²æ£’æ€§",
                "å»ºç«‹æŒç»­å­¦ä¹ æœºåˆ¶ä»¥é€‚åº”æ–°çš„å‘ç¥¨ç±»å‹",
                "ä¼˜åŒ–OCRé¢„å¤„ç†ä»¥å¤„ç†ä¸åŒè´¨é‡çš„æ‰«æä»¶",
                "ä½¿ç”¨ä¸»åŠ¨å­¦ä¹ ç­–ç•¥é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ‡æ³¨æ ·æœ¬"
            ]
            
            for i, suggestion in enumerate(suggestions, 1):
                self.logger.info(f"    {i}. {suggestion}")
                
        except Exception as e:
            self.logger.error(f"  âŒ æ€§èƒ½åˆ†æå¤±è´¥: {e}")


if __name__ == '__main__':
    import multiprocessing
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ï¼Œé˜²æ­¢semaphoreæ³„æ¼
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯
    
    tutorial = InvoiceOCRTutorial()
    tutorial.run_complete_tutorial()